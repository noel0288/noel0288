{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160ddf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b461fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>  this is sample sentence .  <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = ' ' + sentence + ' '      # 이전 스텝에서 본 것처럼 문장 앞뒤로 와 를 단어처럼 붙여 줍니다\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "        \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2736692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66114a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2,figsize = (12, 8))\n",
    "    \n",
    "    for i in range(len(ax)):\n",
    "        ax[i].set_xlabel('Epochs')\n",
    "        ax[i].set_ylabel('Value')\n",
    "        \n",
    "        for n in range(len(list_of_metrics)):\n",
    "            if i == 0:\n",
    "                y = hist[list_of_metrics[n]]\n",
    "                if n == 0:\n",
    "                    ax[i].plot(epochs, y, label=\"train\")\n",
    "                else:\n",
    "                    ax[i].plot(epochs, y, label=\"val\")\n",
    "                ax[i].set_title('Loss')\n",
    "                ax[i].legend(loc='upper right')\n",
    "                if n == 1:\n",
    "                    break\n",
    "            else:\n",
    "                if n >= 2:\n",
    "                    y = hist[list_of_metrics[n]]\n",
    "                    if n == 2:\n",
    "                        ax[i].plot(epochs, y, label=\"train\")\n",
    "                    else:\n",
    "                        ax[i].plot(epochs, y, label=\"val\")\n",
    "                    ax[i].set_title('Accuracy')\n",
    "                    ax[i].legend(loc='lower right')\n",
    "                    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022d629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "        tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b458d4",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8025263e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel//lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442150f8",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b013fe63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>  now i ve heard there was a secret chord  <end>',\n",
       " '<start>  that david played , and it pleased the lord  <end>',\n",
       " '<start>  but you don t really care for music , do you ?  <end>',\n",
       " '<start>  it goes like this  <end>',\n",
       " '<start>  the fourth , the fifth  <end>',\n",
       " '<start>  the minor fall , the major lift  <end>',\n",
       " '<start>  the baffled king composing hallelujah hallelujah  <end>',\n",
       " '<start>  hallelujah  <end>',\n",
       " '<start>  hallelujah  <end>',\n",
       " '<start>  hallelujah your faith was strong but you needed proof  <end>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b0dfa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2639 ...    0    0    0]\n",
      " [   2   36    7 ...   43    3    0]\n",
      " ...\n",
      " [   5   22    9 ...   10 1013    3]\n",
      " [  37   15    1 ...  877  647    3]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f813c5b2e80>\n"
     ]
    }
   ],
   "source": [
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0618dc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    5   91  297   65   57    9  969 6042    3    0    0    0]\n",
      "[  50    5   91  297   65   57    9  969 6042    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 가 아니라 일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc94ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "594110ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:를 포함하여 7001개\n",
    "\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac0a4cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f043d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4ecc6",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1a6e924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.drop1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.drop1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 2048\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e00d5fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 7001), dtype=float32, numpy=\n",
       "array([[[ 2.51601887e-04, -4.01953526e-04,  6.48151501e-04, ...,\n",
       "         -1.76176065e-04, -6.27416070e-04, -4.78025795e-05],\n",
       "        [ 4.25454375e-04, -7.19868869e-04,  1.14288798e-03, ...,\n",
       "         -8.40010005e-04, -1.01470517e-03, -8.59811960e-04],\n",
       "        [ 2.66679359e-04, -3.08028219e-04,  1.01610052e-03, ...,\n",
       "         -1.18735642e-03, -1.31384970e-03, -2.01597996e-03],\n",
       "        ...,\n",
       "        [ 9.44694411e-03, -1.08451364e-04, -9.37328022e-03, ...,\n",
       "          2.00379363e-05, -4.10159410e-04,  3.69950640e-03],\n",
       "        [ 1.10365646e-02, -1.25002710e-03, -1.07907802e-02, ...,\n",
       "         -3.40490486e-04,  2.10244034e-04,  5.46089653e-03],\n",
       "        [ 1.24659063e-02, -2.26646918e-03, -1.19413277e-02, ...,\n",
       "         -6.74556475e-04,  7.98801484e-04,  6.98676426e-03]],\n",
       "\n",
       "       [[ 2.51601887e-04, -4.01953526e-04,  6.48151501e-04, ...,\n",
       "         -1.76176065e-04, -6.27416070e-04, -4.78025795e-05],\n",
       "        [ 4.83540680e-06, -9.68466629e-04,  4.54651687e-04, ...,\n",
       "         -7.62937299e-04, -1.47604721e-03,  1.31445646e-04],\n",
       "        [-3.53229727e-04, -7.83383584e-05,  4.87691577e-04, ...,\n",
       "         -1.27517059e-03, -1.75894427e-04,  6.72337657e-04],\n",
       "        ...,\n",
       "        [ 1.23619614e-03,  3.37566854e-03, -1.36015506e-03, ...,\n",
       "         -2.27190059e-04, -7.56822265e-05,  2.29977234e-03],\n",
       "        [ 3.13935871e-03,  2.45558238e-03, -3.44649027e-03, ...,\n",
       "         -5.28058328e-04, -2.80433669e-05,  3.38556408e-03],\n",
       "        [ 5.09586977e-03,  1.34291523e-03, -5.53578185e-03, ...,\n",
       "         -8.95059551e-04,  2.02745898e-04,  4.57629049e-03]],\n",
       "\n",
       "       [[ 2.51601887e-04, -4.01953526e-04,  6.48151501e-04, ...,\n",
       "         -1.76176065e-04, -6.27416070e-04, -4.78025795e-05],\n",
       "        [-4.01945326e-05, -1.17294875e-03,  8.11298960e-04, ...,\n",
       "         -3.99811572e-04, -1.37104688e-03, -5.69742348e-04],\n",
       "        [-2.20009519e-04, -1.92466530e-03,  1.22410315e-03, ...,\n",
       "         -1.06902572e-03, -1.61851430e-03, -1.37927651e-03],\n",
       "        ...,\n",
       "        [ 7.69476173e-03, -2.30906880e-03, -5.35113737e-03, ...,\n",
       "         -9.62038524e-04,  1.00730069e-03,  1.49373442e-03],\n",
       "        [ 9.37216450e-03, -2.85084103e-03, -7.33184069e-03, ...,\n",
       "         -1.11825007e-03,  1.53985049e-03,  3.18329805e-03],\n",
       "        [ 1.09319128e-02, -3.37355863e-03, -9.02645383e-03, ...,\n",
       "         -1.27632055e-03,  2.07130471e-03,  4.76430496e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.51601887e-04, -4.01953526e-04,  6.48151501e-04, ...,\n",
       "         -1.76176065e-04, -6.27416070e-04, -4.78025795e-05],\n",
       "        [ 8.62383094e-05, -6.85059582e-04,  1.81741593e-03, ...,\n",
       "         -1.98802052e-04, -1.36761006e-03, -6.55043812e-04],\n",
       "        [-2.84025562e-04, -1.09743408e-03,  1.94206845e-03, ...,\n",
       "         -6.11817697e-04, -2.17796187e-03, -8.32377234e-04],\n",
       "        ...,\n",
       "        [ 9.27146710e-03, -1.76090572e-03, -9.14719515e-03, ...,\n",
       "         -4.42425109e-04,  1.79041584e-03,  6.41569775e-03],\n",
       "        [ 1.08671095e-02, -2.41495995e-03, -1.05921784e-02, ...,\n",
       "         -8.27830401e-04,  2.18519382e-03,  7.52718933e-03],\n",
       "        [ 1.22949183e-02, -2.99544493e-03, -1.17596397e-02, ...,\n",
       "         -1.17290963e-03,  2.53453222e-03,  8.50431155e-03]],\n",
       "\n",
       "       [[ 2.51601887e-04, -4.01953526e-04,  6.48151501e-04, ...,\n",
       "         -1.76176065e-04, -6.27416070e-04, -4.78025795e-05],\n",
       "        [ 1.34891743e-04, -2.31670536e-04,  9.25006054e-04, ...,\n",
       "         -7.12420384e-04, -1.00383174e-03, -1.36685895e-03],\n",
       "        [ 6.39017788e-04, -1.63207136e-04,  9.92880436e-04, ...,\n",
       "         -8.07594683e-04, -1.61083776e-03, -1.25599618e-03],\n",
       "        ...,\n",
       "        [ 8.18406045e-03, -2.60666944e-03, -7.63463182e-03, ...,\n",
       "          3.94077040e-04, -1.14432233e-03,  5.54071832e-03],\n",
       "        [ 9.56017803e-03, -3.18402145e-03, -9.28626116e-03, ...,\n",
       "          8.34223283e-06, -5.42678696e-04,  6.69023348e-03],\n",
       "        [ 1.08847013e-02, -3.70708248e-03, -1.06980512e-02, ...,\n",
       "         -3.58724501e-04,  9.11367388e-05,  7.74393044e-03]],\n",
       "\n",
       "       [[ 2.51601887e-04, -4.01953526e-04,  6.48151501e-04, ...,\n",
       "         -1.76176065e-04, -6.27416070e-04, -4.78025795e-05],\n",
       "        [ 2.57108386e-05,  9.76392621e-05,  1.51125365e-03, ...,\n",
       "         -3.33680015e-04, -6.48540678e-04, -7.59458635e-05],\n",
       "        [-5.22019400e-04, -5.65296272e-04,  1.59508793e-03, ...,\n",
       "         -6.46041532e-04, -1.74557463e-05,  2.62086978e-04],\n",
       "        ...,\n",
       "        [ 2.82726157e-03, -1.12799415e-03, -1.81601476e-03, ...,\n",
       "         -8.19162524e-04,  1.59262004e-03,  4.77665616e-03],\n",
       "        [ 4.28052712e-03, -1.62099372e-03, -3.88701470e-03, ...,\n",
       "         -7.95169093e-04,  1.83107925e-03,  5.96485473e-03],\n",
       "        [ 5.91623178e-03, -2.23235949e-03, -5.87975653e-03, ...,\n",
       "         -9.18702281e-04,  2.13883724e-03,  7.10884947e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82527144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  14338048  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  14345049  \n",
      "=================================================================\n",
      "Total params: 95,808,345\n",
      "Trainable params: 95,808,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b2f24e",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3528b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8fe4186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "549/549 [==============================] - 556s 1s/step - loss: 3.1099\n",
      "Epoch 2/3\n",
      "549/549 [==============================] - 439s 798ms/step - loss: 2.5739\n",
      "Epoch 3/3\n",
      "549/549 [==============================] - 264s 481ms/step - loss: 2.2033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7fe0562fa0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# history = model.fit(dataset,\n",
    "          # epochs=7,\n",
    "          # validation_data=val_dataset,\n",
    "          # verbose=1)\n",
    "# optimizer와 loss등은 차차 배웁니다\n",
    "# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n",
    "\n",
    "# Adam 알고리즘을 구현하는 optimzier이며 어떤 optimzier를 써야할지 모른다면 Adam을 쓰는 것도 방법이다.\n",
    "# 우리가 학습을 할 때 최대한 틀리지 않는 방향으로 학습을 해야한다.\n",
    "# 여기서 얼마나 틀리는지(loss)를 알게하는 함수가 손실함수 이다.\n",
    "# 이 손실함수의 최소값을 찾는 것을 학습의 목표로 하며 여기서 최소값을 찾아가는 과정을 optimization 이라하고\n",
    "# 이를 수행하는 알고리즘을 optimizer(최적화)라고 한다.\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam() # Adam은 현재 가장 많이 사용하는 옵티마이저이다. 자세한 내용은 차차 배운다.\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy( # 훈련 데이터의 라벨이 정수의 형태로 제공될 때 사용하는 손실함수이다.\n",
    "    from_logits=True, # 기본값은 False이다. 모델에 의해 생성된 출력 값이 정규화되지 않았음을 손실 함수에 알려준다. 즉 softmax함수가 적용되지 않았다는걸 의미한다. \n",
    "    reduction='none'  # 기본값은 SUM이다. 각자 나오는 값의 반환 원할 때 None을 사용한다.\n",
    ")\n",
    "# 모델을 학습시키키 위한 학습과정을 설정하는 단계이다.\n",
    "model.compile(loss=loss, optimizer=optimizer) # 손실함수와 훈련과정을 설정했다.\n",
    "model.fit(dataset, epochs=3) # 만들어둔 데이터셋으로 모델을 학습한다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e48b83b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot_curve(history.epoch, history.history, ['loss', 'val_loss', 'accuracy', 'val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39eafc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpath = os.getenv('HOME')+'/aiffel//lyricist/model_checkpoint'\n",
    "model.save_weights('mpath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b2d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "# model.load_weights('mpath')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed159277",
   "metadata": {},
   "source": [
    "Generate lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af3941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb51cc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i am a <unk> , <end> '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate_text(model, tokenizer, init_sentence=\" i love\", max_len=20)\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i am\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f54e539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , liberian girl <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate_text(model, tokenizer, init_sentence=\" i love\", max_len=20)\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0966023f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e238b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i want to diamond in the back <end> '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate_text(model, tokenizer, init_sentence=\" i love\", max_len=20)\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i want\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "회고\n",
    "\n",
    "Model\n",
    "LSTM layer 사이에 Dropout(0.3)을 적용하였습니다.\n",
    "embedding_size, hidden_size를 각각 다음과 같이 설정하였습니다.\n",
    "embedding_size = 2048, hidden_size = 2048\n",
    "\n",
    "\n",
    "최종적으로 val_loss를 2.2033 을 얻었습니다.\n",
    "\n",
    "reference\n",
    "https://github.com/JaeHeee/AIFFEL_Project/blob/master/EXPLORATION/EXPLORATION%2011.%20%EC%9E%91%EC%82%AC%EA%B0%80%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EB%A7%8C%EB%93%A4%EA%B8%B0.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c64f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#프로젝트 1 : 손수 설계하는 선형회귀, 당뇨병 수치를 맞춰보자!\n",
    "Import\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "Load data\n",
    "diabetes = load_diabetes()\n",
    "dir(diabetes)\n",
    "['DESCR',\n",
    " 'data',\n",
    " 'data_filename',\n",
    " 'feature_names',\n",
    " 'frame',\n",
    " 'target',\n",
    " 'target_filename']\n",
    "dfX = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "dfy = pd.DataFrame(diabetes.target, columns=['target'])\n",
    "dfX\n",
    "age\tsex\tbmi\tbp\ts1\ts2\ts3\ts4\ts5\ts6\n",
    "0\t0.038076\t0.050680\t0.061696\t0.021872\t-0.044223\t-0.034821\t-0.043401\t-0.002592\t0.019908\t-0.017646\n",
    "1\t-0.001882\t-0.044642\t-0.051474\t-0.026328\t-0.008449\t-0.019163\t0.074412\t-0.039493\t-0.068330\t-0.092204\n",
    "2\t0.085299\t0.050680\t0.044451\t-0.005671\t-0.045599\t-0.034194\t-0.032356\t-0.002592\t0.002864\t-0.025930\n",
    "3\t-0.089063\t-0.044642\t-0.011595\t-0.036656\t0.012191\t0.024991\t-0.036038\t0.034309\t0.022692\t-0.009362\n",
    "4\t0.005383\t-0.044642\t-0.036385\t0.021872\t0.003935\t0.015596\t0.008142\t-0.002592\t-0.031991\t-0.046641\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "437\t0.041708\t0.050680\t0.019662\t0.059744\t-0.005697\t-0.002566\t-0.028674\t-0.002592\t0.031193\t0.007207\n",
    "438\t-0.005515\t0.050680\t-0.015906\t-0.067642\t0.049341\t0.079165\t-0.028674\t0.034309\t-0.018118\t0.044485\n",
    "439\t0.041708\t0.050680\t-0.015906\t0.017282\t-0.037344\t-0.013840\t-0.024993\t-0.011080\t-0.046879\t0.015491\n",
    "440\t-0.045472\t-0.044642\t0.039062\t0.001215\t0.016318\t0.015283\t-0.028674\t0.026560\t0.044528\t-0.025930\n",
    "441\t-0.045472\t-0.044642\t-0.073030\t-0.081414\t0.083740\t0.027809\t0.173816\t-0.039493\t-0.004220\t0.003064\n",
    "442 rows × 10 columns\n",
    "\n",
    "dfy\n",
    "target\n",
    "0\t151.0\n",
    "1\t75.0\n",
    "2\t141.0\n",
    "3\t206.0\n",
    "4\t135.0\n",
    "...\t...\n",
    "437\t178.0\n",
    "438\t104.0\n",
    "439\t132.0\n",
    "440\t220.0\n",
    "441\t57.0\n",
    "442 rows × 1 columns\n",
    "\n",
    "X = dfX.values\n",
    "y = dfy['target'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "Model\n",
    "W = np.random.rand(10)\n",
    "b = np.random.rand()\n",
    "W\n",
    "array([0.27006344, 0.74506016, 0.56395758, 0.8907705 , 0.08553655,\n",
    "       0.50063463, 0.77837307, 0.98421692, 0.84001236, 0.65093915])\n",
    "b\n",
    "0.5320504722847357\n",
    "def model(X,W,b):\n",
    "    predictions = 0\n",
    "    for i in range(10):\n",
    "        predictions += X[:,i]*W[i]\n",
    "    predictions += b\n",
    "    return predictions\n",
    "Loss function\n",
    "def MSE(a,b):\n",
    "    mse = ((a-b)**2).mean()\n",
    "    return mse\n",
    "def loss(X,W,b,y):\n",
    "    predictions = model(X,W,b)\n",
    "    Loss = MSE(predictions,y)\n",
    "    return Loss\n",
    "Gradient function\n",
    "def gradient(X,W,b,y):\n",
    "    N = len(W)\n",
    "    y_pred = model(X,W,b)\n",
    "    \n",
    "    dW = 1/N * 2 * (X.T).dot(y_pred-y)\n",
    "    db = 2 * (y_pred-y).mean()\n",
    "    return dW, db\n",
    "Train\n",
    "learning_rate = 0.001\n",
    "losses = []\n",
    "\n",
    "for i in range(1, 3001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -= learning_rate*dW\n",
    "    b -= learning_rate*db\n",
    "    L = loss(X, W, b, y)\n",
    "    losses.append(L)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))\n",
    "Iteration 10 : Loss 27959.7026\n",
    "Iteration 20 : Loss 27055.6696\n",
    "Iteration 30 : Loss 26186.5425\n",
    "Iteration 40 : Loss 25350.9550\n",
    "Iteration 50 : Loss 24547.5947\n",
    "Iteration 60 : Loss 23775.2007\n",
    "Iteration 70 : Loss 23032.5615\n",
    "Iteration 80 : Loss 22318.5133\n",
    "Iteration 90 : Loss 21631.9379\n",
    "Iteration 100 : Loss 20971.7609\n",
    "Iteration 110 : Loss 20336.9504\n",
    "Iteration 120 : Loss 19726.5147\n",
    "Iteration 130 : Loss 19139.5010\n",
    "Iteration 140 : Loss 18574.9942\n",
    "Iteration 150 : Loss 18032.1148\n",
    "Iteration 160 : Loss 17510.0181\n",
    "Iteration 170 : Loss 17007.8923\n",
    "Iteration 180 : Loss 16524.9576\n",
    "Iteration 190 : Loss 16060.4647\n",
    "Iteration 200 : Loss 15613.6939\n",
    "Iteration 210 : Loss 15183.9536\n",
    "Iteration 220 : Loss 14770.5792\n",
    "Iteration 230 : Loss 14372.9325\n",
    "Iteration 240 : Loss 13990.4001\n",
    "Iteration 250 : Loss 13622.3927\n",
    "Iteration 260 : Loss 13268.3442\n",
    "Iteration 270 : Loss 12927.7106\n",
    "Iteration 280 : Loss 12599.9692\n",
    "Iteration 290 : Loss 12284.6181\n",
    "Iteration 300 : Loss 11981.1748\n",
    "Iteration 310 : Loss 11689.1757\n",
    "Iteration 320 : Loss 11408.1755\n",
    "Iteration 330 : Loss 11137.7464\n",
    "Iteration 340 : Loss 10877.4773\n",
    "Iteration 350 : Loss 10626.9732\n",
    "Iteration 360 : Loss 10385.8546\n",
    "Iteration 370 : Loss 10153.7568\n",
    "Iteration 380 : Loss 9930.3297\n",
    "Iteration 390 : Loss 9715.2365\n",
    "Iteration 400 : Loss 9508.1538\n",
    "Iteration 410 : Loss 9308.7710\n",
    "Iteration 420 : Loss 9116.7895\n",
    "Iteration 430 : Loss 8931.9223\n",
    "Iteration 440 : Loss 8753.8939\n",
    "Iteration 450 : Loss 8582.4395\n",
    "Iteration 460 : Loss 8417.3045\n",
    "Iteration 470 : Loss 8258.2445\n",
    "Iteration 480 : Loss 8105.0245\n",
    "Iteration 490 : Loss 7957.4189\n",
    "Iteration 500 : Loss 7815.2107\n",
    "Iteration 510 : Loss 7678.1915\n",
    "Iteration 520 : Loss 7546.1611\n",
    "Iteration 530 : Loss 7418.9271\n",
    "Iteration 540 : Loss 7296.3045\n",
    "Iteration 550 : Loss 7178.1158\n",
    "Iteration 560 : Loss 7064.1902\n",
    "Iteration 570 : Loss 6954.3637\n",
    "Iteration 580 : Loss 6848.4787\n",
    "Iteration 590 : Loss 6746.3836\n",
    "Iteration 600 : Loss 6647.9331\n",
    "Iteration 610 : Loss 6552.9871\n",
    "Iteration 620 : Loss 6461.4114\n",
    "Iteration 630 : Loss 6373.0768\n",
    "Iteration 640 : Loss 6287.8592\n",
    "Iteration 650 : Loss 6205.6393\n",
    "Iteration 660 : Loss 6126.3026\n",
    "Iteration 670 : Loss 6049.7389\n",
    "Iteration 680 : Loss 5975.8424\n",
    "Iteration 690 : Loss 5904.5115\n",
    "Iteration 700 : Loss 5835.6483\n",
    "Iteration 710 : Loss 5769.1589\n",
    "Iteration 720 : Loss 5704.9532\n",
    "Iteration 730 : Loss 5642.9443\n",
    "Iteration 740 : Loss 5583.0489\n",
    "Iteration 750 : Loss 5525.1869\n",
    "Iteration 760 : Loss 5469.2812\n",
    "Iteration 770 : Loss 5415.2579\n",
    "Iteration 780 : Loss 5363.0459\n",
    "Iteration 790 : Loss 5312.5768\n",
    "Iteration 800 : Loss 5263.7849\n",
    "Iteration 810 : Loss 5216.6071\n",
    "Iteration 820 : Loss 5170.9826\n",
    "Iteration 830 : Loss 5126.8532\n",
    "Iteration 840 : Loss 5084.1628\n",
    "Iteration 850 : Loss 5042.8575\n",
    "Iteration 860 : Loss 5002.8855\n",
    "Iteration 870 : Loss 4964.1969\n",
    "Iteration 880 : Loss 4926.7441\n",
    "Iteration 890 : Loss 4890.4809\n",
    "Iteration 900 : Loss 4855.3631\n",
    "Iteration 910 : Loss 4821.3483\n",
    "Iteration 920 : Loss 4788.3955\n",
    "Iteration 930 : Loss 4756.4655\n",
    "Iteration 940 : Loss 4725.5205\n",
    "Iteration 950 : Loss 4695.5243\n",
    "Iteration 960 : Loss 4666.4418\n",
    "Iteration 970 : Loss 4638.2396\n",
    "Iteration 980 : Loss 4610.8854\n",
    "Iteration 990 : Loss 4584.3482\n",
    "Iteration 1000 : Loss 4558.5982\n",
    "Iteration 1010 : Loss 4533.6066\n",
    "Iteration 1020 : Loss 4509.3460\n",
    "Iteration 1030 : Loss 4485.7898\n",
    "Iteration 1040 : Loss 4462.9125\n",
    "Iteration 1050 : Loss 4440.6896\n",
    "Iteration 1060 : Loss 4419.0976\n",
    "Iteration 1070 : Loss 4398.1138\n",
    "Iteration 1080 : Loss 4377.7164\n",
    "Iteration 1090 : Loss 4357.8844\n",
    "Iteration 1100 : Loss 4338.5977\n",
    "Iteration 1110 : Loss 4319.8369\n",
    "Iteration 1120 : Loss 4301.5833\n",
    "Iteration 1130 : Loss 4283.8191\n",
    "Iteration 1140 : Loss 4266.5269\n",
    "Iteration 1150 : Loss 4249.6902\n",
    "Iteration 1160 : Loss 4233.2930\n",
    "Iteration 1170 : Loss 4217.3200\n",
    "Iteration 1180 : Loss 4201.7563\n",
    "Iteration 1190 : Loss 4186.5879\n",
    "Iteration 1200 : Loss 4171.8009\n",
    "Iteration 1210 : Loss 4157.3823\n",
    "Iteration 1220 : Loss 4143.3195\n",
    "Iteration 1230 : Loss 4129.6001\n",
    "Iteration 1240 : Loss 4116.2126\n",
    "Iteration 1250 : Loss 4103.1456\n",
    "Iteration 1260 : Loss 4090.3884\n",
    "Iteration 1270 : Loss 4077.9303\n",
    "Iteration 1280 : Loss 4065.7615\n",
    "Iteration 1290 : Loss 4053.8723\n",
    "Iteration 1300 : Loss 4042.2532\n",
    "Iteration 1310 : Loss 4030.8955\n",
    "Iteration 1320 : Loss 4019.7904\n",
    "Iteration 1330 : Loss 4008.9296\n",
    "Iteration 1340 : Loss 3998.3052\n",
    "Iteration 1350 : Loss 3987.9094\n",
    "Iteration 1360 : Loss 3977.7349\n",
    "Iteration 1370 : Loss 3967.7745\n",
    "Iteration 1380 : Loss 3958.0213\n",
    "Iteration 1390 : Loss 3948.4688\n",
    "Iteration 1400 : Loss 3939.1105\n",
    "Iteration 1410 : Loss 3929.9403\n",
    "Iteration 1420 : Loss 3920.9524\n",
    "Iteration 1430 : Loss 3912.1409\n",
    "Iteration 1440 : Loss 3903.5005\n",
    "Iteration 1450 : Loss 3895.0258\n",
    "Iteration 1460 : Loss 3886.7117\n",
    "Iteration 1470 : Loss 3878.5534\n",
    "Iteration 1480 : Loss 3870.5461\n",
    "Iteration 1490 : Loss 3862.6852\n",
    "Iteration 1500 : Loss 3854.9664\n",
    "Iteration 1510 : Loss 3847.3854\n",
    "Iteration 1520 : Loss 3839.9382\n",
    "Iteration 1530 : Loss 3832.6208\n",
    "Iteration 1540 : Loss 3825.4294\n",
    "Iteration 1550 : Loss 3818.3603\n",
    "Iteration 1560 : Loss 3811.4101\n",
    "Iteration 1570 : Loss 3804.5753\n",
    "Iteration 1580 : Loss 3797.8527\n",
    "Iteration 1590 : Loss 3791.2390\n",
    "Iteration 1600 : Loss 3784.7312\n",
    "Iteration 1610 : Loss 3778.3265\n",
    "Iteration 1620 : Loss 3772.0219\n",
    "Iteration 1630 : Loss 3765.8146\n",
    "Iteration 1640 : Loss 3759.7021\n",
    "Iteration 1650 : Loss 3753.6818\n",
    "Iteration 1660 : Loss 3747.7513\n",
    "Iteration 1670 : Loss 3741.9080\n",
    "Iteration 1680 : Loss 3736.1498\n",
    "Iteration 1690 : Loss 3730.4745\n",
    "Iteration 1700 : Loss 3724.8798\n",
    "Iteration 1710 : Loss 3719.3636\n",
    "Iteration 1720 : Loss 3713.9241\n",
    "Iteration 1730 : Loss 3708.5593\n",
    "Iteration 1740 : Loss 3703.2672\n",
    "Iteration 1750 : Loss 3698.0461\n",
    "Iteration 1760 : Loss 3692.8943\n",
    "Iteration 1770 : Loss 3687.8100\n",
    "Iteration 1780 : Loss 3682.7916\n",
    "Iteration 1790 : Loss 3677.8376\n",
    "Iteration 1800 : Loss 3672.9464\n",
    "Iteration 1810 : Loss 3668.1165\n",
    "Iteration 1820 : Loss 3663.3465\n",
    "Iteration 1830 : Loss 3658.6350\n",
    "Iteration 1840 : Loss 3653.9807\n",
    "Iteration 1850 : Loss 3649.3823\n",
    "Iteration 1860 : Loss 3644.8385\n",
    "Iteration 1870 : Loss 3640.3481\n",
    "Iteration 1880 : Loss 3635.9100\n",
    "Iteration 1890 : Loss 3631.5230\n",
    "Iteration 1900 : Loss 3627.1860\n",
    "Iteration 1910 : Loss 3622.8979\n",
    "Iteration 1920 : Loss 3618.6577\n",
    "Iteration 1930 : Loss 3614.4644\n",
    "Iteration 1940 : Loss 3610.3170\n",
    "Iteration 1950 : Loss 3606.2146\n",
    "Iteration 1960 : Loss 3602.1562\n",
    "Iteration 1970 : Loss 3598.1410\n",
    "Iteration 1980 : Loss 3594.1681\n",
    "Iteration 1990 : Loss 3590.2367\n",
    "Iteration 2000 : Loss 3586.3459\n",
    "Iteration 2010 : Loss 3582.4951\n",
    "Iteration 2020 : Loss 3578.6833\n",
    "Iteration 2030 : Loss 3574.9099\n",
    "Iteration 2040 : Loss 3571.1741\n",
    "Iteration 2050 : Loss 3567.4753\n",
    "Iteration 2060 : Loss 3563.8127\n",
    "Iteration 2070 : Loss 3560.1858\n",
    "Iteration 2080 : Loss 3556.5938\n",
    "Iteration 2090 : Loss 3553.0361\n",
    "Iteration 2100 : Loss 3549.5121\n",
    "Iteration 2110 : Loss 3546.0213\n",
    "Iteration 2120 : Loss 3542.5631\n",
    "Iteration 2130 : Loss 3539.1369\n",
    "Iteration 2140 : Loss 3535.7421\n",
    "Iteration 2150 : Loss 3532.3782\n",
    "Iteration 2160 : Loss 3529.0448\n",
    "Iteration 2170 : Loss 3525.7412\n",
    "Iteration 2180 : Loss 3522.4671\n",
    "Iteration 2190 : Loss 3519.2219\n",
    "Iteration 2200 : Loss 3516.0052\n",
    "Iteration 2210 : Loss 3512.8165\n",
    "Iteration 2220 : Loss 3509.6554\n",
    "Iteration 2230 : Loss 3506.5215\n",
    "Iteration 2240 : Loss 3503.4143\n",
    "Iteration 2250 : Loss 3500.3334\n",
    "Iteration 2260 : Loss 3497.2784\n",
    "Iteration 2270 : Loss 3494.2490\n",
    "Iteration 2280 : Loss 3491.2447\n",
    "Iteration 2290 : Loss 3488.2652\n",
    "Iteration 2300 : Loss 3485.3102\n",
    "Iteration 2310 : Loss 3482.3792\n",
    "Iteration 2320 : Loss 3479.4719\n",
    "Iteration 2330 : Loss 3476.5880\n",
    "Iteration 2340 : Loss 3473.7271\n",
    "Iteration 2350 : Loss 3470.8890\n",
    "Iteration 2360 : Loss 3468.0732\n",
    "Iteration 2370 : Loss 3465.2796\n",
    "Iteration 2380 : Loss 3462.5078\n",
    "Iteration 2390 : Loss 3459.7575\n",
    "Iteration 2400 : Loss 3457.0284\n",
    "Iteration 2410 : Loss 3454.3203\n",
    "Iteration 2420 : Loss 3451.6328\n",
    "Iteration 2430 : Loss 3448.9657\n",
    "Iteration 2440 : Loss 3446.3187\n",
    "Iteration 2450 : Loss 3443.6916\n",
    "Iteration 2460 : Loss 3441.0841\n",
    "Iteration 2470 : Loss 3438.4959\n",
    "Iteration 2480 : Loss 3435.9269\n",
    "Iteration 2490 : Loss 3433.3767\n",
    "Iteration 2500 : Loss 3430.8452\n",
    "Iteration 2510 : Loss 3428.3321\n",
    "Iteration 2520 : Loss 3425.8372\n",
    "Iteration 2530 : Loss 3423.3602\n",
    "Iteration 2540 : Loss 3420.9010\n",
    "Iteration 2550 : Loss 3418.4593\n",
    "Iteration 2560 : Loss 3416.0350\n",
    "Iteration 2570 : Loss 3413.6277\n",
    "Iteration 2580 : Loss 3411.2374\n",
    "Iteration 2590 : Loss 3408.8638\n",
    "Iteration 2600 : Loss 3406.5068\n",
    "Iteration 2610 : Loss 3404.1661\n",
    "Iteration 2620 : Loss 3401.8415\n",
    "Iteration 2630 : Loss 3399.5329\n",
    "Iteration 2640 : Loss 3397.2401\n",
    "Iteration 2650 : Loss 3394.9629\n",
    "Iteration 2660 : Loss 3392.7011\n",
    "Iteration 2670 : Loss 3390.4546\n",
    "Iteration 2680 : Loss 3388.2232\n",
    "Iteration 2690 : Loss 3386.0068\n",
    "Iteration 2700 : Loss 3383.8051\n",
    "Iteration 2710 : Loss 3381.6180\n",
    "Iteration 2720 : Loss 3379.4453\n",
    "Iteration 2730 : Loss 3377.2870\n",
    "Iteration 2740 : Loss 3375.1428\n",
    "Iteration 2750 : Loss 3373.0126\n",
    "Iteration 2760 : Loss 3370.8962\n",
    "Iteration 2770 : Loss 3368.7936\n",
    "Iteration 2780 : Loss 3366.7045\n",
    "Iteration 2790 : Loss 3364.6288\n",
    "Iteration 2800 : Loss 3362.5664\n",
    "Iteration 2810 : Loss 3360.5171\n",
    "Iteration 2820 : Loss 3358.4809\n",
    "Iteration 2830 : Loss 3356.4575\n",
    "Iteration 2840 : Loss 3354.4469\n",
    "Iteration 2850 : Loss 3352.4489\n",
    "Iteration 2860 : Loss 3350.4634\n",
    "Iteration 2870 : Loss 3348.4902\n",
    "Iteration 2880 : Loss 3346.5293\n",
    "Iteration 2890 : Loss 3344.5806\n",
    "Iteration 2900 : Loss 3342.6438\n",
    "Iteration 2910 : Loss 3340.7189\n",
    "Iteration 2920 : Loss 3338.8058\n",
    "Iteration 2930 : Loss 3336.9044\n",
    "Iteration 2940 : Loss 3335.0145\n",
    "Iteration 2950 : Loss 3333.1360\n",
    "Iteration 2960 : Loss 3331.2689\n",
    "Iteration 2970 : Loss 3329.4130\n",
    "Iteration 2980 : Loss 3327.5682\n",
    "Iteration 2990 : Loss 3325.7344\n",
    "Iteration 3000 : Loss 3323.9115\n",
    "Prediction\n",
    "predictions = model(X_test, W, b)\n",
    "mse = loss(X_test, W, b, y_test)\n",
    "mse\n",
    "3270.8161892345042\n",
    "Visualization\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(X_test[:, 0], y_test, label=\"true\")\n",
    "plt.scatter(X_test[:, 0], predictions, label=\"pred\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "프로젝트 1\n",
    "\n",
    "첫번째 프로젝트는 처음에 학습을 진행했을 때는 원하는 값이 나오지 않았습니다. 그래서, learning rate 와 트레이닝 횟수를 계속 조절하였고, loss값을 3000에 근접하도록 얻을 수 있었습니다.\n",
    "\n",
    "reference\n",
    "https://github.com/JaeHeee/AIFFEL_Project/blob/master/EXPLORATION/EXPLORATION%205.%20%EB%82%A0%EC%94%A8%20%EC%A2%8B%EC%9D%80%20%EC%9B%94%EC%9A%94%EC%9D%BC%20%EC%98%A4%ED%9B%84%20%EC%84%B8%20%EC%8B%9C%2C%20%EC%9E%90%EC%A0%84%EA%B1%B0%20%ED%83%80%EB%8A%94%20%EC%82%AC%EB%9E%8C%EC%9D%80%20%EB%AA%87%20%EB%AA%85%3F.ipynb.\n",
    " \n",
    " \n",
    "#프로젝트 2 : 날씨 좋은 월요일 오후 세 시, 자전거 타는 사람은 몇 명?\n",
    "Import\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "Load data\n",
    "train = pd.read_csv(\"~/aiffel/Project/EXPLORATION/E5/train.csv\")\n",
    "train.head()\n",
    "datetime\tseason\tholiday\tworkingday\tweather\ttemp\tatemp\thumidity\twindspeed\tcasual\tregistered\tcount\n",
    "0\t2011-01-01 00:00:00\t1\t0\t0\t1\t9.84\t14.395\t81\t0.0\t3\t13\t16\n",
    "1\t2011-01-01 01:00:00\t1\t0\t0\t1\t9.02\t13.635\t80\t0.0\t8\t32\t40\n",
    "2\t2011-01-01 02:00:00\t1\t0\t0\t1\t9.02\t13.635\t80\t0.0\t5\t27\t32\n",
    "3\t2011-01-01 03:00:00\t1\t0\t0\t1\t9.84\t14.395\t75\t0.0\t3\t10\t13\n",
    "4\t2011-01-01 04:00:00\t1\t0\t0\t1\t9.84\t14.395\t75\t0.0\t0\t1\t1\n",
    "Create column\n",
    "train['datetime'] = pd.to_datetime(train['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "train['year'] = pd.DatetimeIndex(train['datetime']).year\n",
    "train['month'] = pd.DatetimeIndex(train['datetime']).month\n",
    "train['day'] = pd.DatetimeIndex(train['datetime']).day\n",
    "train['hour'] = pd.DatetimeIndex(train['datetime']).hour\n",
    "train['minute'] = pd.DatetimeIndex(train['datetime']).minute\n",
    "train['second'] = pd.DatetimeIndex(train['datetime']).second\n",
    "train.head()\n",
    "datetime\tseason\tholiday\tworkingday\tweather\ttemp\tatemp\thumidity\twindspeed\tcasual\tregistered\tcount\tyear\tmonth\tday\thour\tminute\tsecond\n",
    "0\t2011-01-01 00:00:00\t1\t0\t0\t1\t9.84\t14.395\t81\t0.0\t3\t13\t16\t2011\t1\t1\t0\t0\t0\n",
    "1\t2011-01-01 01:00:00\t1\t0\t0\t1\t9.02\t13.635\t80\t0.0\t8\t32\t40\t2011\t1\t1\t1\t0\t0\n",
    "2\t2011-01-01 02:00:00\t1\t0\t0\t1\t9.02\t13.635\t80\t0.0\t5\t27\t32\t2011\t1\t1\t2\t0\t0\n",
    "3\t2011-01-01 03:00:00\t1\t0\t0\t1\t9.84\t14.395\t75\t0.0\t3\t10\t13\t2011\t1\t1\t3\t0\t0\n",
    "4\t2011-01-01 04:00:00\t1\t0\t0\t1\t9.84\t14.395\t75\t0.0\t0\t1\t1\t2011\t1\t1\t4\t0\t0\n",
    "Visualization\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2,3,1)\n",
    "sns.countplot(x='year', data=train)\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "sns.countplot(x='month', data=train)\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "sns.countplot(x='day', data=train)\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "sns.countplot(x='hour', data=train)\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "sns.countplot(x='minute', data=train)\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "sns.countplot(x='second', data=train)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "Train Test split\n",
    "X = train[['season', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed', 'year', 'month', 'day', 'hour',]].values\n",
    "y = train[['count']].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "Train\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "LinearRegression()\n",
    "Prediction\n",
    "predictions = model.predict(X_test)\n",
    "predictions\n",
    "array([[-33.96697339],\n",
    "       [ 59.23679918],\n",
    "       [112.62548605],\n",
    "       ...,\n",
    "       [195.79183719],\n",
    "       [197.74006566],\n",
    "       [294.24680107]])\n",
    "rmse = mean_squared_error(y_test, predictions, squared = False)\n",
    "rmse\n",
    "140.47528542723452\n",
    "Visualization\n",
    "col = ['season', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed', 'year', 'month', 'day', 'hour']\n",
    "X_test_df = pd.DataFrame(X_test, columns=col)\n",
    "X_test_df['predictions'] = predictions\n",
    "X_test_df['y_test'] = y_test\n",
    "X_test_df\n",
    "season\tworkingday\tweather\ttemp\tatemp\thumidity\twindspeed\tyear\tmonth\tday\thour\tpredictions\ty_test\n",
    "0\t1.0\t1.0\t2.0\t6.56\t9.090\t80.0\t8.9981\t2011.0\t2.0\t4.0\t9.0\t-33.966973\t127\n",
    "1\t2.0\t1.0\t2.0\t14.76\t16.665\t71.0\t19.0012\t2011.0\t4.0\t8.0\t10.0\t59.236799\t73\n",
    "2\t3.0\t1.0\t3.0\t24.60\t27.275\t88.0\t8.9981\t2011.0\t9.0\t7.0\t9.0\t112.625486\t190\n",
    "3\t2.0\t1.0\t1.0\t16.40\t20.455\t56.0\t23.9994\t2011.0\t4.0\t4.0\t1.0\t44.829705\t11\n",
    "4\t4.0\t1.0\t2.0\t22.14\t25.760\t49.0\t8.9981\t2012.0\t10.0\t17.0\t13.0\t303.039383\t310\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "2173\t1.0\t1.0\t1.0\t12.30\t13.635\t61.0\t19.9995\t2012.0\t3.0\t7.0\t1.0\t76.753725\t5\n",
    "2174\t1.0\t1.0\t1.0\t28.70\t31.820\t39.0\t16.9979\t2012.0\t3.0\t15.0\t17.0\t356.228964\t713\n",
    "2175\t2.0\t1.0\t1.0\t18.86\t22.725\t72.0\t15.0013\t2011.0\t5.0\t6.0\t22.0\t195.791837\t151\n",
    "2176\t1.0\t1.0\t1.0\t12.30\t13.635\t42.0\t31.0009\t2012.0\t1.0\t18.0\t13.0\t197.740066\t130\n",
    "2177\t3.0\t1.0\t1.0\t26.24\t30.305\t65.0\t16.9979\t2011.0\t9.0\t1.0\t23.0\t294.246801\t126\n",
    "2178 rows × 13 columns\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2,2,1)\n",
    "sns.lineplot(x=X_test_df['temp'], y=X_test_df['predictions'])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.lineplot(x=X_test_df['temp'], y=X_test_df['y_test'])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.lineplot(x=X_test_df['humidity'], y=X_test_df['predictions'])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.lineplot(x=X_test_df['humidity'], y=X_test_df['y_test'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "프로젝트 2\n",
    "\n",
    "두번째 프로젝트는 처음에 학습을 진행했을 때, loss 값이 거의 0에 가까운 값이 나왔습니다. 모델이 너무 잘 맞추는 것이 'casual', 'registered' 속성때문이라고 판단되어, 두개의 속성을 제거하였습니다. 추가적으로 'holiday', 'minute', 'second' 속성도 제거하여, 학습을 진행하였습니다. 결과적으로 loss 값이 140.47 정도가 나왔습니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reference\n",
    "https://github.com/JaeHeee/AIFFEL_Project/blob/master/EXPLORATION/EXPLORATION%205.%20%EB%82%A0%EC%94%A8%20%EC%A2%8B%EC%9D%80%20%EC%9B%94%EC%9A%94%EC%9D%BC%20%EC%98%A4%ED%9B%84%20%EC%84%B8%20%EC%8B%9C%2C%20%EC%9E%90%EC%A0%84%EA%B1%B0%20%ED%83%80%EB%8A%94%20%EC%82%AC%EB%9E%8C%EC%9D%80%20%EB%AA%87%20%EB%AA%85%3F.ipynb.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

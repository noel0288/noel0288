{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66fa6488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018118</td>\n",
       "      <td>0.044485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017282</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046879</td>\n",
       "      <td>0.015491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044528</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081414</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>0.003064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809  0.173816   \n",
       "\n",
       "           s4        s5        s6  \n",
       "0   -0.002592  0.019908 -0.017646  \n",
       "1   -0.039493 -0.068330 -0.092204  \n",
       "2   -0.002592  0.002864 -0.025930  \n",
       "3    0.034309  0.022692 -0.009362  \n",
       "4   -0.002592 -0.031991 -0.046641  \n",
       "..        ...       ...       ...  \n",
       "437 -0.002592  0.031193  0.007207  \n",
       "438  0.034309 -0.018118  0.044485  \n",
       "439 -0.011080 -0.046879  0.015491  \n",
       "440  0.026560  0.044528 -0.025930  \n",
       "441 -0.039493 -0.004220  0.003064  \n",
       "\n",
       "[442 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#프로젝트 1 : 손수 설계하는 선형회귀, 당뇨병 수치를 맞춰보자!\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "diabetes = load_diabetes()\n",
    "dir(diabetes)\n",
    "['DESCR',\n",
    " 'data',\n",
    " 'data_filename',\n",
    " 'feature_names',\n",
    " 'frame',\n",
    " 'target',\n",
    " 'target_filename']\n",
    "dfX = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "dfy = pd.DataFrame(diabetes.target, columns=['target'])\n",
    "dfX\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30f2daab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target\n",
       "0     151.0\n",
       "1      75.0\n",
       "2     141.0\n",
       "3     206.0\n",
       "4     135.0\n",
       "..      ...\n",
       "437   178.0\n",
       "438   104.0\n",
       "439   132.0\n",
       "440   220.0\n",
       "441    57.0\n",
       "\n",
       "[442 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2f92d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23765973, 0.84697717, 0.84744284, 0.91372582, 0.98714849,\n",
       "       0.44881253, 0.04492269, 0.08388403, 0.1878953 , 0.16423227])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dfX.values\n",
    "y = dfy['target'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "W = np.random.rand(10)\n",
    "b = np.random.rand()\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8591941f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7094625216809675"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0a8d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X,W,b):\n",
    "    predictions = 0\n",
    "    for i in range(10):\n",
    "        predictions += X[:,i]*W[i]\n",
    "    predictions += b\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffc66751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(a,b):\n",
    "    mse = ((a-b)**2).mean()\n",
    "    return mse\n",
    "def loss(X,W,b,y):\n",
    "    predictions = model(X,W,b)\n",
    "    Loss = MSE(predictions,y)\n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf4670e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X,W,b,y):\n",
    "    N = len(W)\n",
    "    y_pred = model(X,W,b)\n",
    "    \n",
    "    dW = 1/N * 2 * (X.T).dot(y_pred-y)\n",
    "    db = 2 * (y_pred-y).mean()\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dcf560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76b75c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 27910.2024\n",
      "Iteration 20 : Loss 27008.1801\n",
      "Iteration 30 : Loss 26140.9838\n",
      "Iteration 40 : Loss 25307.2507\n",
      "Iteration 50 : Loss 24505.6712\n",
      "Iteration 60 : Loss 23734.9874\n",
      "Iteration 70 : Loss 22993.9905\n",
      "Iteration 80 : Loss 22281.5194\n",
      "Iteration 90 : Loss 21596.4585\n",
      "Iteration 100 : Loss 20937.7359\n",
      "Iteration 110 : Loss 20304.3220\n",
      "Iteration 120 : Loss 19695.2273\n",
      "Iteration 130 : Loss 19109.5014\n",
      "Iteration 140 : Loss 18546.2311\n",
      "Iteration 150 : Loss 18004.5391\n",
      "Iteration 160 : Loss 17483.5824\n",
      "Iteration 170 : Loss 16982.5513\n",
      "Iteration 180 : Loss 16500.6677\n",
      "Iteration 190 : Loss 16037.1841\n",
      "Iteration 200 : Loss 15591.3823\n",
      "Iteration 210 : Loss 15162.5723\n",
      "Iteration 220 : Loss 14750.0912\n",
      "Iteration 230 : Loss 14353.3021\n",
      "Iteration 240 : Loss 13971.5931\n",
      "Iteration 250 : Loss 13604.3761\n",
      "Iteration 260 : Loss 13251.0865\n",
      "Iteration 270 : Loss 12911.1814\n",
      "Iteration 280 : Loss 12584.1395\n",
      "Iteration 290 : Loss 12269.4598\n",
      "Iteration 300 : Loss 11966.6610\n",
      "Iteration 310 : Loss 11675.2806\n",
      "Iteration 320 : Loss 11394.8743\n",
      "Iteration 330 : Loss 11125.0153\n",
      "Iteration 340 : Loss 10865.2934\n",
      "Iteration 350 : Loss 10615.3145\n",
      "Iteration 360 : Loss 10374.7000\n",
      "Iteration 370 : Loss 10143.0861\n",
      "Iteration 380 : Loss 9920.1233\n",
      "Iteration 390 : Loss 9705.4758\n",
      "Iteration 400 : Loss 9498.8209\n",
      "Iteration 410 : Loss 9299.8485\n",
      "Iteration 420 : Loss 9108.2608\n",
      "Iteration 430 : Loss 8923.7717\n",
      "Iteration 440 : Loss 8746.1060\n",
      "Iteration 450 : Loss 8574.9996\n",
      "Iteration 460 : Loss 8410.1986\n",
      "Iteration 470 : Loss 8251.4590\n",
      "Iteration 480 : Loss 8098.5465\n",
      "Iteration 490 : Loss 7951.2358\n",
      "Iteration 500 : Loss 7809.3106\n",
      "Iteration 510 : Loss 7672.5628\n",
      "Iteration 520 : Loss 7540.7929\n",
      "Iteration 530 : Loss 7413.8086\n",
      "Iteration 540 : Loss 7291.4257\n",
      "Iteration 550 : Loss 7173.4668\n",
      "Iteration 560 : Loss 7059.7616\n",
      "Iteration 570 : Loss 6950.1464\n",
      "Iteration 580 : Loss 6844.4641\n",
      "Iteration 590 : Loss 6742.5635\n",
      "Iteration 600 : Loss 6644.2993\n",
      "Iteration 610 : Loss 6549.5321\n",
      "Iteration 620 : Loss 6458.1278\n",
      "Iteration 630 : Loss 6369.9575\n",
      "Iteration 640 : Loss 6284.8974\n",
      "Iteration 650 : Loss 6202.8285\n",
      "Iteration 660 : Loss 6123.6366\n",
      "Iteration 670 : Loss 6047.2116\n",
      "Iteration 680 : Loss 5973.4481\n",
      "Iteration 690 : Loss 5902.2445\n",
      "Iteration 700 : Loss 5833.5035\n",
      "Iteration 710 : Loss 5767.1311\n",
      "Iteration 720 : Loss 5703.0375\n",
      "Iteration 730 : Loss 5641.1360\n",
      "Iteration 740 : Loss 5581.3435\n",
      "Iteration 750 : Loss 5523.5800\n",
      "Iteration 760 : Loss 5467.7687\n",
      "Iteration 770 : Loss 5413.8358\n",
      "Iteration 780 : Loss 5361.7104\n",
      "Iteration 790 : Loss 5311.3241\n",
      "Iteration 800 : Loss 5262.6116\n",
      "Iteration 810 : Loss 5215.5097\n",
      "Iteration 820 : Loss 5169.9579\n",
      "Iteration 830 : Loss 5125.8981\n",
      "Iteration 840 : Loss 5083.2742\n",
      "Iteration 850 : Loss 5042.0326\n",
      "Iteration 860 : Loss 5002.1215\n",
      "Iteration 870 : Loss 4963.4913\n",
      "Iteration 880 : Loss 4926.0942\n",
      "Iteration 890 : Loss 4889.8843\n",
      "Iteration 900 : Loss 4854.8175\n",
      "Iteration 910 : Loss 4820.8514\n",
      "Iteration 920 : Loss 4787.9452\n",
      "Iteration 930 : Loss 4756.0597\n",
      "Iteration 940 : Loss 4725.1573\n",
      "Iteration 950 : Loss 4695.2016\n",
      "Iteration 960 : Loss 4666.1580\n",
      "Iteration 970 : Loss 4637.9928\n",
      "Iteration 980 : Loss 4610.6740\n",
      "Iteration 990 : Loss 4584.1705\n",
      "Iteration 1000 : Loss 4558.4527\n",
      "Iteration 1010 : Loss 4533.4919\n",
      "Iteration 1020 : Loss 4509.2605\n",
      "Iteration 1030 : Loss 4485.7323\n",
      "Iteration 1040 : Loss 4462.8816\n",
      "Iteration 1050 : Loss 4440.6841\n",
      "Iteration 1060 : Loss 4419.1163\n",
      "Iteration 1070 : Loss 4398.1555\n",
      "Iteration 1080 : Loss 4377.7800\n",
      "Iteration 1090 : Loss 4357.9689\n",
      "Iteration 1100 : Loss 4338.7021\n",
      "Iteration 1110 : Loss 4319.9602\n",
      "Iteration 1120 : Loss 4301.7245\n",
      "Iteration 1130 : Loss 4283.9773\n",
      "Iteration 1140 : Loss 4266.7014\n",
      "Iteration 1150 : Loss 4249.8800\n",
      "Iteration 1160 : Loss 4233.4974\n",
      "Iteration 1170 : Loss 4217.5382\n",
      "Iteration 1180 : Loss 4201.9877\n",
      "Iteration 1190 : Loss 4186.8317\n",
      "Iteration 1200 : Loss 4172.0565\n",
      "Iteration 1210 : Loss 4157.6490\n",
      "Iteration 1220 : Loss 4143.5967\n",
      "Iteration 1230 : Loss 4129.8873\n",
      "Iteration 1240 : Loss 4116.5091\n",
      "Iteration 1250 : Loss 4103.4510\n",
      "Iteration 1260 : Loss 4090.7021\n",
      "Iteration 1270 : Loss 4078.2519\n",
      "Iteration 1280 : Loss 4066.0905\n",
      "Iteration 1290 : Loss 4054.2081\n",
      "Iteration 1300 : Loss 4042.5956\n",
      "Iteration 1310 : Loss 4031.2439\n",
      "Iteration 1320 : Loss 4020.1445\n",
      "Iteration 1330 : Loss 4009.2891\n",
      "Iteration 1340 : Loss 3998.6696\n",
      "Iteration 1350 : Loss 3988.2785\n",
      "Iteration 1360 : Loss 3978.1083\n",
      "Iteration 1370 : Loss 3968.1518\n",
      "Iteration 1380 : Loss 3958.4024\n",
      "Iteration 1390 : Loss 3948.8532\n",
      "Iteration 1400 : Loss 3939.4981\n",
      "Iteration 1410 : Loss 3930.3308\n",
      "Iteration 1420 : Loss 3921.3454\n",
      "Iteration 1430 : Loss 3912.5363\n",
      "Iteration 1440 : Loss 3903.8980\n",
      "Iteration 1450 : Loss 3895.4253\n",
      "Iteration 1460 : Loss 3887.1129\n",
      "Iteration 1470 : Loss 3878.9562\n",
      "Iteration 1480 : Loss 3870.9502\n",
      "Iteration 1490 : Loss 3863.0905\n",
      "Iteration 1500 : Loss 3855.3727\n",
      "Iteration 1510 : Loss 3847.7925\n",
      "Iteration 1520 : Loss 3840.3460\n",
      "Iteration 1530 : Loss 3833.0291\n",
      "Iteration 1540 : Loss 3825.8380\n",
      "Iteration 1550 : Loss 3818.7692\n",
      "Iteration 1560 : Loss 3811.8191\n",
      "Iteration 1570 : Loss 3804.9843\n",
      "Iteration 1580 : Loss 3798.2616\n",
      "Iteration 1590 : Loss 3791.6477\n",
      "Iteration 1600 : Loss 3785.1396\n",
      "Iteration 1610 : Loss 3778.7344\n",
      "Iteration 1620 : Loss 3772.4293\n",
      "Iteration 1630 : Loss 3766.2214\n",
      "Iteration 1640 : Loss 3760.1083\n",
      "Iteration 1650 : Loss 3754.0872\n",
      "Iteration 1660 : Loss 3748.1558\n",
      "Iteration 1670 : Loss 3742.3116\n",
      "Iteration 1680 : Loss 3736.5524\n",
      "Iteration 1690 : Loss 3730.8760\n",
      "Iteration 1700 : Loss 3725.2802\n",
      "Iteration 1710 : Loss 3719.7629\n",
      "Iteration 1720 : Loss 3714.3222\n",
      "Iteration 1730 : Loss 3708.9560\n",
      "Iteration 1740 : Loss 3703.6626\n",
      "Iteration 1750 : Loss 3698.4402\n",
      "Iteration 1760 : Loss 3693.2869\n",
      "Iteration 1770 : Loss 3688.2012\n",
      "Iteration 1780 : Loss 3683.1813\n",
      "Iteration 1790 : Loss 3678.2258\n",
      "Iteration 1800 : Loss 3673.3330\n",
      "Iteration 1810 : Loss 3668.5015\n",
      "Iteration 1820 : Loss 3663.7299\n",
      "Iteration 1830 : Loss 3659.0168\n",
      "Iteration 1840 : Loss 3654.3608\n",
      "Iteration 1850 : Loss 3649.7607\n",
      "Iteration 1860 : Loss 3645.2153\n",
      "Iteration 1870 : Loss 3640.7232\n",
      "Iteration 1880 : Loss 3636.2833\n",
      "Iteration 1890 : Loss 3631.8945\n",
      "Iteration 1900 : Loss 3627.5558\n",
      "Iteration 1910 : Loss 3623.2659\n",
      "Iteration 1920 : Loss 3619.0239\n",
      "Iteration 1930 : Loss 3614.8288\n",
      "Iteration 1940 : Loss 3610.6796\n",
      "Iteration 1950 : Loss 3606.5753\n",
      "Iteration 1960 : Loss 3602.5151\n",
      "Iteration 1970 : Loss 3598.4981\n",
      "Iteration 1980 : Loss 3594.5234\n",
      "Iteration 1990 : Loss 3590.5901\n",
      "Iteration 2000 : Loss 3586.6975\n",
      "Iteration 2010 : Loss 3582.8448\n",
      "Iteration 2020 : Loss 3579.0312\n",
      "Iteration 2030 : Loss 3575.2559\n",
      "Iteration 2040 : Loss 3571.5183\n",
      "Iteration 2050 : Loss 3567.8176\n",
      "Iteration 2060 : Loss 3564.1532\n",
      "Iteration 2070 : Loss 3560.5244\n",
      "Iteration 2080 : Loss 3556.9305\n",
      "Iteration 2090 : Loss 3553.3710\n",
      "Iteration 2100 : Loss 3549.8453\n",
      "Iteration 2110 : Loss 3546.3526\n",
      "Iteration 2120 : Loss 3542.8926\n",
      "Iteration 2130 : Loss 3539.4645\n",
      "Iteration 2140 : Loss 3536.0679\n",
      "Iteration 2150 : Loss 3532.7022\n",
      "Iteration 2160 : Loss 3529.3670\n",
      "Iteration 2170 : Loss 3526.0616\n",
      "Iteration 2180 : Loss 3522.7857\n",
      "Iteration 2190 : Loss 3519.5387\n",
      "Iteration 2200 : Loss 3516.3203\n",
      "Iteration 2210 : Loss 3513.1298\n",
      "Iteration 2220 : Loss 3509.9670\n",
      "Iteration 2230 : Loss 3506.8313\n",
      "Iteration 2240 : Loss 3503.7223\n",
      "Iteration 2250 : Loss 3500.6397\n",
      "Iteration 2260 : Loss 3497.5830\n",
      "Iteration 2270 : Loss 3494.5519\n",
      "Iteration 2280 : Loss 3491.5459\n",
      "Iteration 2290 : Loss 3488.5647\n",
      "Iteration 2300 : Loss 3485.6079\n",
      "Iteration 2310 : Loss 3482.6752\n",
      "Iteration 2320 : Loss 3479.7663\n",
      "Iteration 2330 : Loss 3476.8807\n",
      "Iteration 2340 : Loss 3474.0182\n",
      "Iteration 2350 : Loss 3471.1784\n",
      "Iteration 2360 : Loss 3468.3611\n",
      "Iteration 2370 : Loss 3465.5659\n",
      "Iteration 2380 : Loss 3462.7924\n",
      "Iteration 2390 : Loss 3460.0405\n",
      "Iteration 2400 : Loss 3457.3099\n",
      "Iteration 2410 : Loss 3454.6001\n",
      "Iteration 2420 : Loss 3451.9111\n",
      "Iteration 2430 : Loss 3449.2424\n",
      "Iteration 2440 : Loss 3446.5939\n",
      "Iteration 2450 : Loss 3443.9653\n",
      "Iteration 2460 : Loss 3441.3562\n",
      "Iteration 2470 : Loss 3438.7666\n",
      "Iteration 2480 : Loss 3436.1960\n",
      "Iteration 2490 : Loss 3433.6444\n",
      "Iteration 2500 : Loss 3431.1114\n",
      "Iteration 2510 : Loss 3428.5968\n",
      "Iteration 2520 : Loss 3426.1004\n",
      "Iteration 2530 : Loss 3423.6220\n",
      "Iteration 2540 : Loss 3421.1614\n",
      "Iteration 2550 : Loss 3418.7183\n",
      "Iteration 2560 : Loss 3416.2925\n",
      "Iteration 2570 : Loss 3413.8839\n",
      "Iteration 2580 : Loss 3411.4922\n",
      "Iteration 2590 : Loss 3409.1172\n",
      "Iteration 2600 : Loss 3406.7588\n",
      "Iteration 2610 : Loss 3404.4167\n",
      "Iteration 2620 : Loss 3402.0908\n",
      "Iteration 2630 : Loss 3399.7808\n",
      "Iteration 2640 : Loss 3397.4867\n",
      "Iteration 2650 : Loss 3395.2082\n",
      "Iteration 2660 : Loss 3392.9451\n",
      "Iteration 2670 : Loss 3390.6973\n",
      "Iteration 2680 : Loss 3388.4646\n",
      "Iteration 2690 : Loss 3386.2469\n",
      "Iteration 2700 : Loss 3384.0439\n",
      "Iteration 2710 : Loss 3381.8556\n",
      "Iteration 2720 : Loss 3379.6817\n",
      "Iteration 2730 : Loss 3377.5221\n",
      "Iteration 2740 : Loss 3375.3766\n",
      "Iteration 2750 : Loss 3373.2452\n",
      "Iteration 2760 : Loss 3371.1276\n",
      "Iteration 2770 : Loss 3369.0238\n",
      "Iteration 2780 : Loss 3366.9335\n",
      "Iteration 2790 : Loss 3364.8566\n",
      "Iteration 2800 : Loss 3362.7931\n",
      "Iteration 2810 : Loss 3360.7426\n",
      "Iteration 2820 : Loss 3358.7052\n",
      "Iteration 2830 : Loss 3356.6807\n",
      "Iteration 2840 : Loss 3354.6690\n",
      "Iteration 2850 : Loss 3352.6698\n",
      "Iteration 2860 : Loss 3350.6832\n",
      "Iteration 2870 : Loss 3348.7090\n",
      "Iteration 2880 : Loss 3346.7470\n",
      "Iteration 2890 : Loss 3344.7971\n",
      "Iteration 2900 : Loss 3342.8593\n",
      "Iteration 2910 : Loss 3340.9333\n",
      "Iteration 2920 : Loss 3339.0191\n",
      "Iteration 2930 : Loss 3337.1166\n",
      "Iteration 2940 : Loss 3335.2257\n",
      "Iteration 2950 : Loss 3333.3462\n",
      "Iteration 2960 : Loss 3331.4780\n",
      "Iteration 2970 : Loss 3329.6211\n",
      "Iteration 2980 : Loss 3327.7753\n",
      "Iteration 2990 : Loss 3325.9405\n",
      "Iteration 3000 : Loss 3324.1166\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 3001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -= learning_rate*dW\n",
    "    b -= learning_rate*db\n",
    "    L = loss(X, W, b, y)\n",
    "    losses.append(L)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25bdf540",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1356572059.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_44/1356572059.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    감사드립니다.\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#  회고 처음아라도 여기 까지 왔습나다. 참으로 기대가 됩니다. 데이타를 가져오고, 모델에 입력할 X를 준바, y준비 등이 너무나 새이경스럽기 까지 합니다.\n",
    "감사드립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a43e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#프로젝트 2 : 날씨 좋은 월요일 오후 세 시, 자전거 타는 사람은 몇 명?\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d829f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"~/data/data/bike-sharing-demand/train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['datetime'] = pd.to_datetime(train['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "train['year'] = pd.DatetimeIndex(train['datetime']).year\n",
    "train['month'] = pd.DatetimeIndex(train['datetime']).month\n",
    "train['day'] = pd.DatetimeIndex(train['datetime']).day\n",
    "train['hour'] = pd.DatetimeIndex(train['datetime']).hour\n",
    "train['minute'] = pd.DatetimeIndex(train['datetime']).minute\n",
    "train['second'] = pd.DatetimeIndex(train['datetime']).second\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17161820",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44/1201398566.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2,3,1)\n",
    "sns.countplot(x='year', data=train)\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "sns.countplot(x='month', data=train)\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "sns.countplot(x='day', data=train)\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "sns.countplot(x='hour', data=train)\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "sns.countplot(x='minute', data=train)\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "sns.countplot(x='second', data=train)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "261fc285",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44/937562999.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'season'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'workingday'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weather'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'temp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'atemp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'humidity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'windspeed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hour'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "X = train[['season', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed', 'year', 'month', 'day', 'hour',]].values\n",
    "y = train[['count']].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f5c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "회고 2 \n",
    "train = pd.read_csv(\"~/data/data/bike-sharing-demand/train.csv\")\n",
    "train.head()\n",
    "와 ~/data/data/bike-sharing-demand 경로에 train.csv 를 이해하지 못하여 더 공부해야 합니다.\n",
    "여기 까지 온 것도 감사드립니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
